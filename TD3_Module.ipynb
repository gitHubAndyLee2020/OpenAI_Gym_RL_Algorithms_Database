{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOe0ObfQHS3tXYH3r2nBgKl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitHubAndyLee2020/OpenAI_Gym_RL_Algorithms_Database/blob/main/TD3_Module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TD3\n",
        "\n",
        "> About\n",
        "\n",
        "- Consists of Actor and two Q networks.\n",
        "- For continuous action space.\n",
        "- Actor: state -> action value\n",
        "- Q Network: state + action -> expected reward\n",
        "\n",
        "> Model Dependency\n",
        "\n",
        "- Actor + Target Q Networks (discounted reward) -> Q Networks; Q Network No.1 (delayed for stability) -> Actor; Q Networks (update) -> Target Q Networks\n",
        "\n",
        "> Pro\n",
        "\n",
        "- Stability and Robustness\n",
        "\n",
        "> Con\n",
        "\n",
        "- Sample Inefficiency"
      ],
      "metadata": {
        "id": "P1FgBK-pIAuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class Replay_buffer():\n",
        "  def __init__(self, max_size=capacity):\n",
        "    - Initialize transition storage\n",
        "\n",
        "  def push(self, data):\n",
        "    - Insert the data into the storage; if the storage is full, replace the oldest data in the storage\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    - Select some number of ranom indexes from the storage, and return the tensors for state, action, next state, done, and reward for those indexes\n",
        "```"
      ],
      "metadata": {
        "id": "QMd77m-r_f1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class Actor(nn.Module):\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    - Map state -> hidden layer -> action values\n",
        "\n",
        "  def forward(self, state):\n",
        "    - Feed the input state through the neural network and transform the range of the output action values to be between +-max action value\n",
        "```"
      ],
      "metadata": {
        "id": "9DTdUO8BAAWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class Critic(nn.Module):\n",
        "  # The model is named Critic in the codebase, but it's essentially a Q network\n",
        "\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    - Map state + action -> hidden layer -> expected reward\n",
        "\n",
        "  def forward(self, state, action):\n",
        "    - Feed the input state and action through the neural network and return the output expected reward\n",
        "```"
      ],
      "metadata": {
        "id": "qNRW7dpiAYDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class TD3():\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    - Initialize the Actor, Critic No.1, Critic No.2, Actor Target, Critic No.1 Target, Critic No.2 Target networks\n",
        "    - Initialize the optimizers for Actor, Critic No.1, Critic No.2 networks\n",
        "    - Copy the weights from Actor, Critic No.1, Critic No.2 to their target networks\n",
        "    - Initialize the transition storage\n",
        "\n",
        "  def select_action(self, state):\n",
        "    - Feed the state to Actor network and return the resulting action value\n",
        "\n",
        "  def update(self, num_iteration):\n",
        "    - For some number of iterations, repeat the following update loop\n",
        "    # Update Loop\n",
        "    - 1. Select sample state, action, next state, done, and reward tensors from the transition storage\n",
        "    - 2. Get the action for the next state by feeding next state to the Actor network, and add noise (tensor with same dimension as action, normally distributed with clipped values) to stable training\n",
        "    - 3. Feed the next state and next action from above to each target Critic No.1 and No.2, and take the minimum Q value, and then calculate the discounted reward using formula target_Q = reward + gamma * target_Q\n",
        "    # Adding noise (likely to be sub-optimal) and taking the minimum of the Q value prevents the target_Q from being too high, preventing the Q networks from overpredicting\n",
        "    - 4. Get the current Q value by feeding state and action to each Critic No.1 and No.2, and use the MSE of the current Q value and target Q value as loss value to backpropagate the Critic networks (guides to Critic Q networks to make more accurate predictions)\n",
        "    - 5. Only every some number of iterations, update the Actor network. This is to make sure the Critic networks are accurate enough before updating the Actor network, which helps to stablise the training\n",
        "    - 6. Use the negated expected value from Critic No.1 as Actor loss; this represents how well the Actor is expected to be performining; the negation is specifically for the Pendulum-v1 environment where the rewards are negative, meaning better performance will result in smaller loss\n",
        "    - 7. Copy tau amount of network from Actor, Critic No.1, and Critic No.2 networks to their target counterparts\n",
        "\n",
        "  def save(self):\n",
        "    - Save the weights of the networks\n",
        "\n",
        "  def load(self):\n",
        "    - Load weights into the networks\n",
        "```"
      ],
      "metadata": {
        "id": "bw9Bee9WBAMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def main():\n",
        "  - If the mode is test, for some number of iterations, play the game until the end, and print the average steps and reward\n",
        "  - Otherwise if the mode is train, for some number of iterations, run the training loop\n",
        "  - In each training loop, collect transition data from the environment, and update the model when the storage fills up. Terminate the current training loop if the game ends and start the next training loop"
      ],
      "metadata": {
        "id": "1gU4hKhIHdFo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxh-_5c5-tYj"
      },
      "outputs": [],
      "source": [
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "\n",
        "import os, sys, random\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "# Hard-coded variables\n",
        "mode = 'train'\n",
        "env_name = \"Pendulum-v1\"\n",
        "tau = 0.005\n",
        "target_update_interval = 1\n",
        "iteration = 5\n",
        "learning_rate = 3e-4\n",
        "gamma = 0.99\n",
        "capacity = 50000\n",
        "num_iteration = 100000\n",
        "batch_size = 100\n",
        "seed = 1\n",
        "num_hidden_layers = 2\n",
        "sample_frequency = 256\n",
        "activation = 'Relu'\n",
        "render = False\n",
        "log_interval = 50\n",
        "load = False\n",
        "render_interval = 100\n",
        "policy_noise = 0.2\n",
        "noise_clip = 0.5\n",
        "policy_delay = 2\n",
        "exploration_noise = 0.1\n",
        "max_episode = 2000\n",
        "print_log = 5\n",
        "\n",
        "# Set seeds\n",
        "# env.seed(seed)\n",
        "# torch.manual_seed(seed)\n",
        "# np.random.seed(seed)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "script_name = \"td3\"\n",
        "env = gym.make(env_name)\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "min_Val = torch.tensor(1e-7).float().to(device) # min value\n",
        "\n",
        "directory = './exp' + script_name + env_name +'./'\n",
        "'''\n",
        "Implementation of TD3 with pytorch\n",
        "Original paper: https://arxiv.org/abs/1802.09477\n",
        "Not the author's implementation !\n",
        "'''\n",
        "\n",
        "class Replay_buffer():\n",
        "    '''\n",
        "    Code based on:\n",
        "    https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
        "    Expects tuples of (state, next_state, action, reward, done)\n",
        "    '''\n",
        "    def __init__(self, max_size=capacity):\n",
        "        self.storage = []\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "\n",
        "    def push(self, data):\n",
        "        if len(self.storage) == self.max_size:\n",
        "            self.storage[int(self.ptr)] = data\n",
        "            self.ptr = (self.ptr + 1) % self.max_size\n",
        "        else:\n",
        "            self.storage.append(data)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "        x, y, u, r, d = [], [], [], [], []\n",
        "\n",
        "        for i in ind:\n",
        "            X, Y, U, R, D = self.storage[i]\n",
        "            x.append(np.array(X, copy=False))\n",
        "            y.append(np.array(Y, copy=False))\n",
        "            u.append(np.array(U, copy=False))\n",
        "            r.append(np.array(R, copy=False))\n",
        "            d.append(np.array(D, copy=False))\n",
        "\n",
        "        return np.array(x), np.array(y), np.array(u), np.array(r).reshape(-1, 1), np.array(d).reshape(-1, 1)\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim, 400)\n",
        "        self.fc2 = nn.Linear(400, 300)\n",
        "        self.fc3 = nn.Linear(300, action_dim)\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, state):\n",
        "        a = F.relu(self.fc1(state))\n",
        "        a = F.relu(self.fc2(a))\n",
        "        a = torch.tanh(self.fc3(a)) * self.max_action\n",
        "        return a\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.fc2 = nn.Linear(400, 300)\n",
        "        self.fc3 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        state_action = torch.cat([state, action], 1)\n",
        "\n",
        "        q = F.relu(self.fc1(state_action))\n",
        "        q = F.relu(self.fc2(q))\n",
        "        q = self.fc3(q)\n",
        "        return q\n",
        "\n",
        "\n",
        "class TD3():\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.critic_1 = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_1_target = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_2 = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_2_target = Critic(state_dim, action_dim).to(device)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters())\n",
        "        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters())\n",
        "        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters())\n",
        "\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n",
        "        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.memory = Replay_buffer(capacity)\n",
        "        self.writer = SummaryWriter(directory)\n",
        "        self.num_critic_update_iteration = 0\n",
        "        self.num_actor_update_iteration = 0\n",
        "        self.num_training = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.tensor(state.reshape(1, -1)).float().to(device)\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "    def update(self, num_iteration):\n",
        "\n",
        "        if self.num_training % 500 == 0:\n",
        "            print(\"====================================\")\n",
        "            print(\"model has been trained for {} times...\".format(self.num_training))\n",
        "            print(\"====================================\")\n",
        "        for i in range(num_iteration):\n",
        "            x, y, u, r, d = self.memory.sample(batch_size)\n",
        "            state = torch.FloatTensor(x).to(device)\n",
        "            action = torch.FloatTensor(u).to(device)\n",
        "            next_state = torch.FloatTensor(y).to(device)\n",
        "            done = torch.FloatTensor(d).to(device)\n",
        "            reward = torch.FloatTensor(r).to(device)\n",
        "\n",
        "            # Select next action according to target policy:\n",
        "            noise = torch.ones_like(action).data.normal_(0, policy_noise).to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            next_action = (self.actor_target(next_state) + noise)\n",
        "            next_action = next_action.clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            # Compute target Q-value:\n",
        "            target_Q1 = self.critic_1_target(next_state, next_action)\n",
        "            target_Q2 = self.critic_2_target(next_state, next_action)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            target_Q = reward + ((1 - done) * gamma * target_Q).detach()\n",
        "\n",
        "            # Optimize Critic 1:\n",
        "            current_Q1 = self.critic_1(state, action)\n",
        "            loss_Q1 = F.mse_loss(current_Q1, target_Q)\n",
        "            self.critic_1_optimizer.zero_grad()\n",
        "            loss_Q1.backward()\n",
        "            self.critic_1_optimizer.step()\n",
        "            self.writer.add_scalar('Loss/Q1_loss', loss_Q1, global_step=self.num_critic_update_iteration)\n",
        "\n",
        "            # Optimize Critic 2:\n",
        "            current_Q2 = self.critic_2(state, action)\n",
        "            loss_Q2 = F.mse_loss(current_Q2, target_Q)\n",
        "            self.critic_2_optimizer.zero_grad()\n",
        "            loss_Q2.backward()\n",
        "            self.critic_2_optimizer.step()\n",
        "            self.writer.add_scalar('Loss/Q2_loss', loss_Q2, global_step=self.num_critic_update_iteration)\n",
        "            # Delayed policy updates:\n",
        "            if i % policy_delay == 0:\n",
        "                # Compute actor loss:\n",
        "                actor_loss = - self.critic_1(state, self.actor(state)).mean()\n",
        "\n",
        "                # Optimize the actor\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()\n",
        "                self.writer.add_scalar('Loss/actor_loss', actor_loss, global_step=self.num_actor_update_iteration)\n",
        "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                    target_param.data.copy_(((1- tau) * target_param.data) + tau * param.data)\n",
        "\n",
        "                for param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n",
        "                    target_param.data.copy_(((1 - tau) * target_param.data) + tau * param.data)\n",
        "\n",
        "                for param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n",
        "                    target_param.data.copy_(((1 - tau) * target_param.data) + tau * param.data)\n",
        "\n",
        "                self.num_actor_update_iteration += 1\n",
        "        self.num_critic_update_iteration += 1\n",
        "        self.num_training += 1\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.actor.state_dict(), directory+'actor.pth')\n",
        "        torch.save(self.actor_target.state_dict(), directory+'actor_target.pth')\n",
        "        torch.save(self.critic_1.state_dict(), directory+'critic_1.pth')\n",
        "        torch.save(self.critic_1_target.state_dict(), directory+'critic_1_target.pth')\n",
        "        torch.save(self.critic_2.state_dict(), directory+'critic_2.pth')\n",
        "        torch.save(self.critic_2_target.state_dict(), directory+'critic_2_target.pth')\n",
        "        print(\"====================================\")\n",
        "        print(\"Model has been saved...\")\n",
        "        print(\"====================================\")\n",
        "\n",
        "    def load(self):\n",
        "        self.actor.load_state_dict(torch.load(directory + 'actor.pth'))\n",
        "        self.actor_target.load_state_dict(torch.load(directory + 'actor_target.pth'))\n",
        "        self.critic_1.load_state_dict(torch.load(directory + 'critic_1.pth'))\n",
        "        self.critic_1_target.load_state_dict(torch.load(directory + 'critic_1_target.pth'))\n",
        "        self.critic_2.load_state_dict(torch.load(directory + 'critic_2.pth'))\n",
        "        self.critic_2_target.load_state_dict(torch.load(directory + 'critic_2_target.pth'))\n",
        "        print(\"====================================\")\n",
        "        print(\"model has been loaded...\")\n",
        "        print(\"====================================\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    agent = TD3(state_dim, action_dim, max_action)\n",
        "    ep_r = 0\n",
        "\n",
        "    if mode == 'test':\n",
        "        agent.load()\n",
        "        for i in range(iteration):\n",
        "            state = env.reset()\n",
        "            for t in count():\n",
        "                action = agent.select_action(state)\n",
        "                next_state, reward, done, info = env.step(np.float32(action))\n",
        "                ep_r += reward\n",
        "                env.render()\n",
        "                if done or t ==2000 :\n",
        "                    print(\"Ep_i \\t{}, the ep_r is \\t{:0.2f}, the step is \\t{}\".format(i, ep_r, t))\n",
        "                    break\n",
        "                state = next_state\n",
        "\n",
        "    elif mode == 'train':\n",
        "        print(\"====================================\")\n",
        "        print(\"Collection Experience...\")\n",
        "        print(\"====================================\")\n",
        "        if load: agent.load()\n",
        "        for i in range(num_iteration):\n",
        "            state = env.reset()\n",
        "            for t in range(2000):\n",
        "\n",
        "                action = agent.select_action(state)\n",
        "                action = action + np.random.normal(0, exploration_noise, size=env.action_space.shape[0])\n",
        "                action = action.clip(env.action_space.low, env.action_space.high)\n",
        "                next_state, reward, done, info = env.step(action)\n",
        "                ep_r += reward\n",
        "                if render and i >= render_interval : env.render()\n",
        "                agent.memory.push((state, next_state, action, reward, np.float(done)))\n",
        "                if i+1 % 10 == 0:\n",
        "                    print('Episode {},  The memory size is {} '.format(i, len(agent.memory.storage)))\n",
        "                if len(agent.memory.storage) >= capacity-1:\n",
        "                    agent.update(10)\n",
        "\n",
        "                state = next_state\n",
        "                if done or t == max_episode -1:\n",
        "                    agent.writer.add_scalar('ep_r', ep_r, global_step=i)\n",
        "                    if i % print_log == 0:\n",
        "                        print(\"Ep_i \\t{}, the ep_r is \\t{:0.2f}, the step is \\t{}\".format(i, ep_r, t))\n",
        "                    ep_r = 0\n",
        "                    break\n",
        "\n",
        "            if i % log_interval == 0:\n",
        "                agent.save()\n",
        "\n",
        "    else:\n",
        "        raise NameError(\"mode wrong!!!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}