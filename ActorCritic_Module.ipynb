{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7E8ubkd7KGIC0Fjl9pIbj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitHubAndyLee2020/OpenAI_Gym_RL_Algorithms_Database/blob/main/ActorCritic_Module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actor-Critic\n",
        "\n",
        "> About\n",
        "\n",
        "- Given some state, the Actor-Critic model will output the probability of taking each possible action, and expected reward for the state\n",
        "- The difference between the actual reward from taking the selected action and the expected reward is used to calculate the loss value, which trains the neural network to maximize the probability of action that produces the maximum reward\n",
        "- Same concept as Policy Gradient, except the expected reward stabilizes the training by reducing the variance of the loss value\n",
        "\n",
        "> Pro\n",
        "\n",
        "- Variance Reduction; more stable training than Policy Gradient\n",
        "\n",
        "> Con\n",
        "\n",
        "- Sensitivity to Hyperparameters; compared to Policy Gradient"
      ],
      "metadata": {
        "id": "5FE8WTLK18cW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class Policy(nn.Module):\n",
        "  def __init__(self):\n",
        "    - Initialize neural network that has two mappings, one that maps state space to action space and another that maps state space to state value\n",
        "    - The state value represents the estimated reward given the current state\n",
        "    - Initialize storage for saving actions and rewards\n",
        "\n",
        "  def forward(self, x):\n",
        "    - Feed the state input through both state-to-action-probability mapping and state-to-state-value mapping\n",
        "    - Apply softmax to generated action probability and return the action probability and state value\n",
        "```"
      ],
      "metadata": {
        "id": "yEbRltCYslq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def select_action(state):\n",
        "  - Feed the state to the Policy network to get action probability and state value\n",
        "  - Select an action from the action probabilty; if action had p probability in the action probabilty, then it has p chance of being selected\n",
        "  - Store the selected action and state value pair in the storage\n",
        "  - Return the selected action\n",
        "```"
      ],
      "metadata": {
        "id": "o6JLAuf1twjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def finish_episode():\n",
        "  - Calculate the accumulated reward with formula R = r0 + gamma * r1 + gamma^2 * r2 + ...\n",
        "  - Normalize the reward\n",
        "  - Calculate the policy loss using the following steps:\n",
        "    1. Compute the difference between actual reward and state value, a.k.a. predicted reward. The difference represents the advantage of the action compared to the average reward expected\n",
        "    2. Loss is calculated by the negated log probability multiplied by the advantage. Higher advantage results in higher loss, which means the optimizer will adjust the neural network more towards the direction of producing higher probability for high-reward actions\n",
        "  - Backpropagation is applied\n",
        "```"
      ],
      "metadata": {
        "id": "eUc32Z9fx-OG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def main():\n",
        "  - During the data collection loop, selected actions and rewards are stored\n",
        "  - The data collection loop runs until the agent fails in the environment\n",
        "  - Afterwards, the collected data is used for the finish_episode functon defined above, where the model is trained\n",
        "```"
      ],
      "metadata": {
        "id": "GSy39s2E0-bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import gym, os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import count\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Create a CartPole-v0 environment object from the OpenAI Gym library\n",
        "env = gym.make('CartPole-v0')\n",
        "# Remove any wrappers from the environment\n",
        "env = env.unwrapped\n",
        "\n",
        "# Seed the environment and PyTorch random number generator for reproducibility\n",
        "env.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Get the size of the state and action spaces\n",
        "state_space = env.observation_space.shape[0]\n",
        "action_space = env.action_space.n\n",
        "\n",
        "# Set hyperparameters\n",
        "learning_rate = 0.01\n",
        "gamma = 0.99\n",
        "episodes = 20000\n",
        "render = False\n",
        "eps = np.finfo(np.float32).eps.item()\n",
        "\n",
        "# Define a Named Tuple to store log probabilities and values for the actions\n",
        "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
        "\n",
        "# Define the neural network model for the policy\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self):\n",
        "        # Call parent class constructor\n",
        "        super(Policy, self).__init__()\n",
        "        # Define the first fully-connected layer\n",
        "        self.fc1 = nn.Linear(state_space, 32)\n",
        "\n",
        "        # Define the action and value heads for the neural network\n",
        "        self.action_head = nn.Linear(32, action_space)\n",
        "        self.value_head = nn.Linear(32, 1)  # Scalar Value\n",
        "\n",
        "        # Initialize lists to store actions and rewards\n",
        "        self.save_actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "        # Create a directory to store outputs\n",
        "        os.makedirs('./AC_CartPole-v0', exist_ok=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the neural network\n",
        "        x = F.relu(self.fc1(x))\n",
        "        action_score = self.action_head(x)\n",
        "        state_value = self.value_head(x)\n",
        "\n",
        "        return F.softmax(action_score, dim=-1), state_value\n",
        "\n",
        "# Initialize the policy model and the optimizer\n",
        "model = Policy()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Function to plot training progress\n",
        "def plot(steps):\n",
        "    ax = plt.subplot(111)\n",
        "    ax.cla()\n",
        "    ax.grid()\n",
        "    ax.set_title('Training')\n",
        "    ax.set_xlabel('Episode')\n",
        "    ax.set_ylabel('Run Time')\n",
        "    ax.plot(steps)\n",
        "    RunTime = len(steps)\n",
        "\n",
        "    path = './AC_CartPole-v0/' + 'RunTime' + str(RunTime) + '.jpg'\n",
        "    if len(steps) % 200 == 0:\n",
        "        plt.savefig(path)\n",
        "    plt.pause(0.0000001)\n",
        "\n",
        "# Function to select an action based on the current state\n",
        "def select_action(state):\n",
        "    state = torch.from_numpy(state).float()\n",
        "    probs, state_value = model(state)\n",
        "    m = Categorical(probs)\n",
        "    action = m.sample()\n",
        "    model.save_actions.append(SavedAction(m.log_prob(action), state_value))\n",
        "\n",
        "    return action.item()\n",
        "\n",
        "# Function to optimize the policy network\n",
        "def finish_episode():\n",
        "    R = 0\n",
        "    save_actions = model.save_actions\n",
        "    policy_loss = []\n",
        "    value_loss = []\n",
        "    rewards = []\n",
        "\n",
        "    for r in model.rewards[::-1]:\n",
        "        R = r + gamma * R\n",
        "        rewards.insert(0, R)\n",
        "\n",
        "    rewards = torch.tensor(rewards)\n",
        "    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)\n",
        "\n",
        "    for (log_prob , value), r in zip(save_actions, rewards):\n",
        "        reward = r - value.item()\n",
        "        policy_loss.append(-log_prob * reward)\n",
        "        value_loss.append(F.smooth_l1_loss(value, torch.tensor([r])))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = torch.stack(policy_loss).sum() + torch.stack(value_loss).sum()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    del model.rewards[:]\n",
        "    del model.save_actions[:]\n",
        "\n",
        "# Main function where the training loop exists\n",
        "def main():\n",
        "    running_reward = 10\n",
        "    live_time = []\n",
        "\n",
        "    # Loop through episodes\n",
        "    for i_episode in count(episodes):\n",
        "        state = env.reset()\n",
        "\n",
        "        # Loop through time steps in each episode\n",
        "        for t in count():\n",
        "            action = select_action(state)\n",
        "            state, reward, _, done, info = env.step(action)\n",
        "            if render: env.render()\n",
        "            model.rewards.append(reward)\n",
        "\n",
        "            if done or t >= 1000:\n",
        "                break\n",
        "\n",
        "        # Update running reward and plot the progress\n",
        "        running_reward = running_reward * 0.99 + t * 0.01\n",
        "        live_time.append(t)\n",
        "        plot(live_time)\n",
        "\n",
        "        # Save the model every 100 episodes\n",
        "        if i_episode % 100 == 0:\n",
        "            modelPath = './AC_CartPole_Model/ModelTraing'+str(i_episode)+'Times.pkl'\n",
        "            torch.save(model, modelPath)\n",
        "\n",
        "        # Optimize the policy\n",
        "        finish_episode()\n",
        "\n",
        "# Entry point of the script\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "MsgxalCmIEMc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}