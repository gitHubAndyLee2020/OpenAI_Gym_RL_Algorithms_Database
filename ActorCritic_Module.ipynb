{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzhOFDCzC5XxGJDtmiLFf2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitHubAndyLee2020/OpenAI_Gym_RL_Algorithms_Database/blob/main/ActorCritic_Module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actor-Critic\n",
        "\n",
        "> About\n",
        "\n",
        "- Given some state, the Actor-Critic model will output the probability of taking each possible action, and expected reward for the state\n",
        "- The difference between the actual reward from taking the selected action and the expected reward is used to calculate the loss value, which trains the neural network to maximize the probability of action that produces the maximum reward\n",
        "- Same concept as Policy Gradient, except the expected reward stabilizes the training by reducing the variance of the loss value\n",
        "\n",
        "> Pro\n",
        "\n",
        "- Variance Reduction; more stable training than Policy Gradient\n",
        "\n",
        "> Con\n",
        "\n",
        "- Sensitivity to Hyperparameters; compared to Policy Gradient"
      ],
      "metadata": {
        "id": "5FE8WTLK18cW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class Policy(nn.Module):\n",
        "  def __init__(self):\n",
        "    - Initialize neural network that has two mappings, one that maps state space to action space and another that maps state space to state value\n",
        "    - The state value represents the estimated reward given the current state\n",
        "    - Initialize storage for saving actions and rewards\n",
        "\n",
        "  def forward(self, x):\n",
        "    - Feed the state input through both state-to-action-probability mapping and state-to-state-value mapping\n",
        "    - Apply softmax to generated action probability and return the action probability and state value\n",
        "```"
      ],
      "metadata": {
        "id": "yEbRltCYslq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def select_action(state):\n",
        "  - Feed the state to the Policy network to get action probability and state value\n",
        "  - Select an action from the action probabilty; if action had p probability in the action probabilty, then it has p chance of being selected\n",
        "  - Store the selected action and state value pair in the storage\n",
        "  - Return the selected action\n",
        "```"
      ],
      "metadata": {
        "id": "o6JLAuf1twjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def finish_episode():\n",
        "  - Calculate the accumulated reward with formula R = r0 + gamma * r1 + gamma^2 * r2 + ...\n",
        "  - Normalize the reward\n",
        "  - Calculate the policy loss using the following steps:\n",
        "    1. Compute the difference between actual reward and state value, a.k.a. predicted reward. The difference represents the advantage of the action compared to the average reward expected\n",
        "    2. The reward is then multiplied by the negated log probability of the selected action to get the loss value, where higher reward and higher action log probability results in lower loss value, which means the neural network will be more adjusted to produce actions that result in higher reward\n",
        "  - Backpropagation is applied\n",
        "```"
      ],
      "metadata": {
        "id": "eUc32Z9fx-OG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def main():\n",
        "  - During the data collection loop, selected actions and rewards are stored\n",
        "  - The data collection loop runs until the agent fails in the environment\n",
        "  - Afterwards, the collected data is used for the finish_episode functon defined above, where the model is trained\n",
        "```"
      ],
      "metadata": {
        "id": "GSy39s2E0-bt"
      }
    }
  ]
}