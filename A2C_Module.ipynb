{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvalgnXmnLzABryyvoTE03",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitHubAndyLee2020/OpenAI_Gym_RL_Algorithms_Database/blob/main/A2C_Module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A2C\n",
        "\n",
        "> About\n",
        "\n",
        "- Given some state, the A2C model will output the probability of taking each possible action, and the expected reward for the state\n",
        "- The A2C model is applied to multiple environments with different starting state to collect more data in parallel\n",
        "- The difference between the actual reward from taking the selected action and the expected reward is used to calculate the loss value for each environment, and the mean of the loss is used to train the A2C model. This trains the neural network to maximize the probability of action that produces the maximum reward\n",
        "- Same concept as Actor-Critic, except the Actor-Critic network is applied to multiple environments simultaneously.\n",
        "\n",
        "> Pro\n",
        "\n",
        "- Parallelism; more data collected simultaneously\n",
        "\n",
        "> Con\n",
        "\n",
        "- Complexity"
      ],
      "metadata": {
        "id": "ADYOzMjrcc-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Assume multiprocessing_env module is implemented already\n",
        "```"
      ],
      "metadata": {
        "id": "Lz8CfULGKlpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def make_env():\n",
        "  def _thunk():\n",
        "    - Create environment and return it\n",
        "  - Return the function to create environment\n",
        "```"
      ],
      "metadata": {
        "id": "cCjWAedSKmnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Before model definition\n",
        "- Initialize multiple environments\n",
        "- Convert them into multi-processed environments\n",
        "- Define test environment\n",
        "```"
      ],
      "metadata": {
        "id": "dA0EynZ6KzqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class ActorCritic(nn.Module):\n",
        "  def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
        "    - Define critic network that maps inputs -> hidden layer -> estimated reward\n",
        "    - Define actor network that maps inputs -> hidden layer -> action probabilities\n",
        "\n",
        "  def forward(self, x):\n",
        "    - Feed the input through the critic network and actor network, and convert the action probabilities to categorical distribution (ex: P(A) = 0.2, P(B) = 0.3, etc...)\n",
        "    - Return the estimated reward and categorical action probabilities\n",
        "```"
      ],
      "metadata": {
        "id": "cZNYim-iLP6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def test_env(vis=False):\n",
        "  - Test the trained model on the test environment\n",
        "  - Run the model on the environment with the actions generated by the Actor-Critic network until the game is done\n",
        "  - Return the total reward\n",
        "```"
      ],
      "metadata": {
        "id": "O6vDkxBZMKnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
        "  - Compute R = r0 + gamma * r1 + gamma^2 * r2 + ... for all rewards, apply mask to ignore rewards after the agent is done. The next_value is the last reward value\n",
        "  - Return the computed return values\n",
        "```"
      ],
      "metadata": {
        "id": "2rfNSiCqMyYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Run the following training loop and post training loop until the frame count of the agent playing the environment exceeds the max frame limit\n",
        "# Miltiple environment is used during data collection to speed up the process\n",
        "# Pre Training Loop\n",
        "- Reset the multiple environments, each environment will have slightly different starting state\n",
        "# Main Training Loop\n",
        "- For x number of steps, feed the state of each environment to the Actor-Critic network, and select the action from the action categorical probability\n",
        "- Apply the each action to its environment, and get the resulting reward for each environment\n",
        "- Store the reward, action log probability, entropy, potential reward, and mask for each environment. Mask is a list of 0s and 1s where its 0 at \"done\", and 1 otherwise. This acts to ignore rewards after the agent is done in the environment. Entropy is the measure of uncertainty and randomness in the distribution\n",
        "- Every y frames, run the test_env function above to test the performance of the current Actor-Critic network\n",
        "# Post Traning Loop\n",
        "- Continuing from the last state from each of the environment from the training loop, feed it to the Actor-Critic network and get the expected reward values; named \"next_value\" in the code\n",
        "- Use the \"next_value\", collected rewards, and masks to calculate the temporarily discounted return value from compute_returns function from above\n",
        "- Calculate the difference between the discounted returns values and collected expected values to get the advantage for each environment, which represents how well each environment performed with the current Actor compared to the expected return that the critic expected\n",
        "- Actor loss is calculated by the mean of each negated log probability multiplied by each advantage. Higher advantage results in higher loss, which means the optimizer will adjust the neural network more towards the direction of producing higher probability for high-reward actions\n",
        "- Critic loss is calculated by taking the mean of the square of the advantages. This represents the Mean Squared Error of the Critic network, where the network will try to minimize this value\n",
        "- Actor loss and Critic loss is combined, and subtract by entropy. Higher advantage leads to both higher Actor loss and Critic loss, which works for both Actor network and Critic network (Actor network wants to move more towards favoring high-reward actions, Critic network wants to adjust more for more wrong predictions).\n",
        "- Entropy is subtracted from the combined loss, because (1) exploration is encouraged. (2) High entropy means more uncertainty in the model choosing the actions. (2) Higher entropy decreases the loss value to maintain the current status-quo of exploratory nature\n",
        "- Apply backpropagation\n",
        "```"
      ],
      "metadata": {
        "id": "ePAmEpWFNBRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Why does higher advantage leads to higher loss, and why does higher loss lead to the model favoring thoses actions?\n",
        "# A higher positive advantage makes the loss term more positive.\n",
        "Since the optimizer aims to minimize this loss, it will adjust the parameters such that log p(a∣s) becomes less negative, effectively making p(a∣s) larger. This means the probability of taking that action a in state s will increase, making it more likely to be chosen in the future.\n",
        "```"
      ],
      "metadata": {
        "id": "0Atv-fulTzvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from multiprocessing import Process, Pipe\n",
        "\n",
        "def worker(remote, parent_remote, env_fn_wrapper):\n",
        "    parent_remote.close()\n",
        "    env = env_fn_wrapper.x()\n",
        "    while True:\n",
        "        cmd, data = remote.recv()\n",
        "        if cmd == 'step':\n",
        "            ob, reward, done, info = env.step(data)\n",
        "            if done:\n",
        "                ob = env.reset()\n",
        "            remote.send((ob, reward, done, info))\n",
        "        elif cmd == 'reset':\n",
        "            ob = env.reset()\n",
        "            remote.send(ob)\n",
        "        elif cmd == 'reset_task':\n",
        "            ob = env.reset_task()\n",
        "            remote.send(ob)\n",
        "        elif cmd == 'close':\n",
        "            remote.close()\n",
        "            break\n",
        "        elif cmd == 'get_spaces':\n",
        "            remote.send((env.observation_space, env.action_space))\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "class VecEnv(object):\n",
        "    \"\"\"\n",
        "    An abstract asynchronous, vectorized environment.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_envs, observation_space, action_space):\n",
        "        self.num_envs = num_envs\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset all the environments and return an array of\n",
        "        observations, or a tuple of observation arrays.\n",
        "        If step_async is still doing work, that work will\n",
        "        be cancelled and step_wait() should not be called\n",
        "        until step_async() is invoked again.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def step_async(self, actions):\n",
        "        \"\"\"\n",
        "        Tell all the environments to start taking a step\n",
        "        with the given actions.\n",
        "        Call step_wait() to get the results of the step.\n",
        "        You should not call this if a step_async run is\n",
        "        already pending.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def step_wait(self):\n",
        "        \"\"\"\n",
        "        Wait for the step taken with step_async().\n",
        "        Returns (obs, rews, dones, infos):\n",
        "         - obs: an array of observations, or a tuple of\n",
        "                arrays of observations.\n",
        "         - rews: an array of rewards\n",
        "         - dones: an array of \"episode done\" booleans\n",
        "         - infos: a sequence of info objects\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"\n",
        "        Clean up the environments' resources.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def step(self, actions):\n",
        "        self.step_async(actions)\n",
        "        return self.step_wait()\n",
        "\n",
        "\n",
        "class CloudpickleWrapper(object):\n",
        "    \"\"\"\n",
        "    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n",
        "    \"\"\"\n",
        "    def __init__(self, x):\n",
        "        self.x = x\n",
        "    def __getstate__(self):\n",
        "        import cloudpickle\n",
        "        return cloudpickle.dumps(self.x)\n",
        "    def __setstate__(self, ob):\n",
        "        import pickle\n",
        "        self.x = pickle.loads(ob)\n",
        "\n",
        "\n",
        "class SubprocVecEnv(VecEnv):\n",
        "    def __init__(self, env_fns, spaces=None):\n",
        "        \"\"\"\n",
        "        envs: list of gym environments to run in subprocesses\n",
        "        \"\"\"\n",
        "        self.waiting = False\n",
        "        self.closed = False\n",
        "        nenvs = len(env_fns)\n",
        "        self.nenvs = nenvs\n",
        "        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n",
        "        self.ps = [Process(target=worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))\n",
        "            for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]\n",
        "        for p in self.ps:\n",
        "            p.daemon = True # if the main process crashes, we should not cause things to hang\n",
        "            p.start()\n",
        "        for remote in self.work_remotes:\n",
        "            remote.close()\n",
        "\n",
        "        self.remotes[0].send(('get_spaces', None))\n",
        "        observation_space, action_space = self.remotes[0].recv()\n",
        "        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n",
        "\n",
        "    def step_async(self, actions):\n",
        "        for remote, action in zip(self.remotes, actions):\n",
        "            remote.send(('step', action))\n",
        "        self.waiting = True\n",
        "\n",
        "    def step_wait(self):\n",
        "        results = [remote.recv() for remote in self.remotes]\n",
        "        self.waiting = False\n",
        "        obs, rews, dones, infos = zip(*results)\n",
        "        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n",
        "\n",
        "    def reset(self):\n",
        "        for remote in self.remotes:\n",
        "            remote.send(('reset', None))\n",
        "        return np.stack([remote.recv() for remote in self.remotes])\n",
        "\n",
        "    def reset_task(self):\n",
        "        for remote in self.remotes:\n",
        "            remote.send(('reset_task', None))\n",
        "        return np.stack([remote.recv() for remote in self.remotes])\n",
        "\n",
        "    def close(self):\n",
        "        if self.closed:\n",
        "            return\n",
        "        if self.waiting:\n",
        "            for remote in self.remotes:\n",
        "                remote.recv()\n",
        "        for remote in self.remotes:\n",
        "            remote.send(('close', None))\n",
        "        for p in self.ps:\n",
        "            p.join()\n",
        "            self.closed = True\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nenvs"
      ],
      "metadata": {
        "id": "oLRMm43IP4fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary modules\n",
        "import math\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if CUDA is available and set the device accordingly\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "# Importing SubprocVecEnv for parallelizing environments\n",
        "# from multiprocessing_env import SubprocVecEnv\n",
        "\n",
        "# Number of parallel environments and name of the environment\n",
        "num_envs = 8\n",
        "env_name = \"CartPole-v0\"\n",
        "\n",
        "# Function to create a new environment\n",
        "def make_env():\n",
        "    def _thunk():\n",
        "        env = gym.make(env_name)\n",
        "        return env\n",
        "    return _thunk\n",
        "\n",
        "# Enabling interactive mode for plotting\n",
        "plt.ion()\n",
        "\n",
        "# Create multiple environments for parallel execution\n",
        "envs = [make_env() for i in range(num_envs)]\n",
        "envs = SubprocVecEnv(envs)  # Create 8 parallel environments\n",
        "\n",
        "# Create a single environment for testing\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# Define the ActorCritic neural network model\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        # Define the critic network\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(num_inputs, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "\n",
        "        # Define the actor network\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(num_inputs, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, num_outputs),\n",
        "            nn.Softmax(dim=1),\n",
        "        )\n",
        "\n",
        "    # Forward pass for both actor and critic\n",
        "    def forward(self, x):\n",
        "        value = self.critic(x)\n",
        "        probs = self.actor(x)\n",
        "        dist = Categorical(probs)\n",
        "        return dist, value\n",
        "\n",
        "# Function to test the trained model in a single environment\n",
        "def test_env(vis=False):\n",
        "    state = env.reset()\n",
        "    if vis: env.render()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        dist, _ = model(state)\n",
        "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
        "        state = next_state\n",
        "        if vis: env.render()\n",
        "        total_reward += reward\n",
        "    return total_reward\n",
        "\n",
        "# Function to compute the discounted returns\n",
        "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
        "    R = next_value\n",
        "    returns = []\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        R = rewards[step] + gamma * R * masks[step]\n",
        "        returns.insert(0, R)\n",
        "    return returns\n",
        "\n",
        "# Function to plot rewards during training\n",
        "def plot(frame_idx, rewards):\n",
        "    plt.plot(rewards, 'b-')\n",
        "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
        "    plt.pause(0.0001)\n",
        "\n",
        "# Define model's input and output dimensions\n",
        "num_inputs = envs.observation_space.shape[0]\n",
        "num_outputs = envs.action_space.n\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 256\n",
        "lr = 1e-3\n",
        "num_steps = 5\n",
        "\n",
        "# Initialize the ActorCritic model and optimizer\n",
        "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training setup\n",
        "max_frames = 20000\n",
        "frame_idx = 0\n",
        "test_rewards = []\n",
        "\n",
        "# Initial state reset\n",
        "state = envs.reset()\n",
        "\n",
        "# Main training loop\n",
        "while frame_idx < max_frames:\n",
        "    # Initialize lists to store trajectories\n",
        "    log_probs = []\n",
        "    values = []\n",
        "    rewards = []\n",
        "    masks = []\n",
        "    entropy = 0\n",
        "\n",
        "    # Rollout trajectory for a few steps\n",
        "    for _ in range(num_steps):\n",
        "\n",
        "        # Convert the state to a PyTorch float tensor and move it to the configured device (CPU or GPU).\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "\n",
        "        # Get the action distribution ('dist') and state value ('value') from the Actor-Critic model.\n",
        "        dist, value = model(state)\n",
        "\n",
        "        # Sample an action from the distribution.\n",
        "        action = dist.sample()\n",
        "\n",
        "        # Take the action in the environment and receive the next state, reward, and 'done' flag.\n",
        "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
        "\n",
        "        # Calculate the log probability of the taken action.\n",
        "        log_prob = dist.log_prob(action)\n",
        "\n",
        "        # Compute the entropy for the action distribution to encourage exploration.\n",
        "        entropy += dist.entropy().mean()\n",
        "\n",
        "        # Append the log probability, state value, reward, and mask to their respective lists.\n",
        "        log_probs.append(log_prob)\n",
        "        values.append(value)\n",
        "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
        "\n",
        "        # Create a mask for the 'done' flag and append it to the masks list.\n",
        "        # The mask will be 0 if the episode is done, otherwise 1.\n",
        "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
        "\n",
        "        # Update the current state to be the next state.\n",
        "        state = next_state\n",
        "\n",
        "        # Increment the frame index.\n",
        "        frame_idx += 1\n",
        "\n",
        "        # Test and plot the performance every 100 frames.\n",
        "        if frame_idx % 100 == 0:\n",
        "\n",
        "            # Evaluate the model's performance in the environment and take the mean over 10 runs.\n",
        "            test_rewards.append(np.mean([test_env() for _ in range(10)]))\n",
        "\n",
        "            # Plot the accumulated test rewards.\n",
        "            plot(frame_idx, test_rewards)\n",
        "\n",
        "    # Compute discounted returns\n",
        "    next_state = torch.FloatTensor(next_state).to(device)\n",
        "    _, next_value = model(next_state)\n",
        "    returns = compute_returns(next_value, rewards, masks)\n",
        "\n",
        "    # Convert lists to PyTorch tensors\n",
        "    log_probs = torch.cat(log_probs)\n",
        "    returns = torch.cat(returns).detach()\n",
        "    values = torch.cat(values)\n",
        "\n",
        "    # Calculate advantage and losses\n",
        "    advantage = returns - values\n",
        "    actor_loss = -(log_probs * advantage.detach()).mean()\n",
        "    critic_loss = advantage.pow(2).mean()\n",
        "\n",
        "    # Calculate total loss and perform backpropagation\n",
        "    loss = actor_loss + 0.5 * critic_loss - 0.001 * entropy\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#test_env(True)"
      ],
      "metadata": {
        "id": "WH_3y61zIS_5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}