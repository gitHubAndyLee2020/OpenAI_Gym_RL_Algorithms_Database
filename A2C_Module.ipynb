{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9aBjjeUZRYZHSmNNUZVzl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitHubAndyLee2020/OpenAI_Gym_RL_Algorithms_Database/blob/main/A2C_Module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A2C\n",
        "\n",
        "> About\n",
        "\n",
        "- Given some state, the A2C model will output the probability of taking each possible action, and the expected reward for the state\n",
        "- The A2C model is applied to multiple environments with different starting state to collect more data in parallel\n",
        "- The difference between the actual reward from taking the selected action and the expected reward is used to calculate the loss value for each environment, and the mean of the loss is used to train the A2C model. This trains the neural network to maximize the probability of action that produces the maximum reward\n",
        "- Same concept as Actor-Critic, except the Actor-Critic network is applied to multiple environments simultaneously.\n",
        "\n",
        "> Pro\n",
        "\n",
        "- Parallelism; more data collected simultaneously\n",
        "\n",
        "> Con\n",
        "\n",
        "- Complexity"
      ],
      "metadata": {
        "id": "ADYOzMjrcc-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Assume multiprocessing_env module is implemented already\n",
        "```"
      ],
      "metadata": {
        "id": "Lz8CfULGKlpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def make_env():\n",
        "  def _thunk():\n",
        "    - Create environment and return it\n",
        "  - Return the function to create environment\n",
        "```"
      ],
      "metadata": {
        "id": "cCjWAedSKmnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Before model definition\n",
        "- Initialize multiple environments\n",
        "- Convert them into multi-processed environments\n",
        "- Define test environment\n",
        "```"
      ],
      "metadata": {
        "id": "dA0EynZ6KzqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class ActorCritic(nn.Module):\n",
        "  def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
        "    - Define critic network that maps inputs -> hidden layer -> estimated reward\n",
        "    - Define actor network that maps inputs -> hidden layer -> action probabilities\n",
        "\n",
        "  def forward(self, x):\n",
        "    - Feed the input through the critic network and actor network, and convert the action probabilities to categorical distribution (ex: P(A) = 0.2, P(B) = 0.3, etc...)\n",
        "    - Return the estimated reward and categorical action probabilities\n",
        "```"
      ],
      "metadata": {
        "id": "cZNYim-iLP6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def test_env(vis=False):\n",
        "  - Test the trained model on the test environment\n",
        "  - Run the model on the environment with the actions generated by the Actor-Critic network until the game is done\n",
        "  - Return the total reward\n",
        "```"
      ],
      "metadata": {
        "id": "O6vDkxBZMKnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
        "  - Compute R = r0 + gamma * r1 + gamma^2 * r2 + ... for all rewards, apply mask to ignore rewards after the agent is done. The next_value is the last reward value\n",
        "  - Return the computed return values\n",
        "```"
      ],
      "metadata": {
        "id": "2rfNSiCqMyYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Run the following training loop and post training loop until the frame count of the agent playing the environment exceeds the max frame limit\n",
        "# Miltiple environment is used during data collection to speed up the process\n",
        "# Pre Training Loop\n",
        "- Reset the multiple environments, each environment will have slightly different starting state\n",
        "# Main Training Loop\n",
        "- For x number of steps, feed the state of each environment to the Actor-Critic network, and select the action from the action categorical probability\n",
        "- Apply the each action to its environment, and get the resulting reward for each environment\n",
        "- Store the reward, action log probability, entropy, potential reward, and mask for each environment. Mask is a list of 0s and 1s where its 0 at \"done\", and 1 otherwise. This acts to ignore rewards after the agent is done in the environment. Entropy is the measure of uncertainty and randomness in the distribution\n",
        "- Every y frames, run the test_env function above to test the performance of the current Actor-Critic network\n",
        "# Post Traning Loop\n",
        "- Continuing from the last state from each of the environment from the training loop, feed it to the Actor-Critic network and get the expected reward values; named \"next_value\" in the code\n",
        "- Use the \"next_value\", collected rewards, and masks to calculate the temporarily discounted return value from compute_returns function from above\n",
        "- Calculate the difference between the discounted returns values and collected expected values to get the advantage for each environment, which represents how well each environment performed with the current Actor compared to the expected return that the critic expected\n",
        "- Actor loss is calculated by the mean of each negated log probability multiplied by each advantage. Higher advantage results in higher loss, which means the optimizer will adjust the neural network more towards the direction of producing higher probability for high-reward actions\n",
        "- Critic loss is calculated by taking the mean of the square of the advantages. This represents the Mean Squared Error of the Critic network, where the network will try to minimize this value\n",
        "- Actor loss and Critic loss is combined, and subtract by entropy. Higher advantage leads to both higher Actor loss and Critic loss, which works for both Actor network and Critic network (Actor network wants to move more towards favoring high-reward actions, Critic network wants to adjust more for more wrong predictions).\n",
        "- Entropy is subtracted from the combined loss, because (1) exploration is encouraged. (2) High entropy means more uncertainty in the model choosing the actions. (2) Higher entropy decreases the loss value to maintain the current status-quo of exploratory nature\n",
        "- Apply backpropagation\n",
        "```"
      ],
      "metadata": {
        "id": "ePAmEpWFNBRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Why does higher advantage leads to higher loss, and why does higher loss lead to the model favoring thoses actions?\n",
        "# A higher positive advantage makes the loss term more positive.\n",
        "Since the optimizer aims to minimize this loss, it will adjust the parameters such that log p(a∣s) becomes less negative, effectively making p(a∣s) larger. This means the probability of taking that action a in state s will increase, making it more likely to be chosen in the future.\n",
        "```"
      ],
      "metadata": {
        "id": "0Atv-fulTzvt"
      }
    }
  ]
}