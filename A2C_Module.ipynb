{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNj8CMulPB50eG8oG7Y2hwh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitHubAndyLee2020/OpenAI_Gym_RL_Algorithms_Database/blob/main/A2C_Module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A2C\n",
        "\n",
        "> About\n",
        "\n",
        "- Given some state, the A2C model will output the probability of taking each possible action, and the expected reward for the state\n",
        "- The A2C model is applied to multiple environments with different starting state to collect more data in parallel\n",
        "- The difference between the actual reward from taking the selected action and the expected reward is used to calculate the loss value for each environment, and the mean of the loss is used to train the A2C model. This trains the neural network to maximize the probability of action that produces the maximum reward\n",
        "- Same concept as Actor-Critic, except the Actor-Critic network is applied to multiple environments simultaneously.\n",
        "\n",
        "> Pro\n",
        "\n",
        "- Parallelism; more data collected simultaneously\n",
        "\n",
        "> Con\n",
        "\n",
        "- Complexity"
      ],
      "metadata": {
        "id": "ADYOzMjrcc-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Assume multiprocessing_env module is implemented already\n",
        "```"
      ],
      "metadata": {
        "id": "Lz8CfULGKlpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def make_env():\n",
        "  def _thunk():\n",
        "    - Create environment and return it\n",
        "  - Return the function to create environment\n",
        "```"
      ],
      "metadata": {
        "id": "cCjWAedSKmnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Before model definition\n",
        "- Initialize multiple environments\n",
        "- Convert them into multi-processed environments\n",
        "- Define test environment\n",
        "```"
      ],
      "metadata": {
        "id": "dA0EynZ6KzqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class ActorCritic(nn.Module):\n",
        "  def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
        "    - Define critic network that maps inputs -> hidden layer -> estimated reward\n",
        "    - Define actor network that maps inputs -> hidden layer -> action probabilities\n",
        "\n",
        "  def forward(self, x):\n",
        "    - Feed the input through the critic network and actor network, and convert the action probabilities to categorical distribution (ex: P(A) = 0.2, P(B) = 0.3, etc...)\n",
        "    - Return the estimated reward and categorical action probabilities\n",
        "```"
      ],
      "metadata": {
        "id": "cZNYim-iLP6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def test_env(vis=False):\n",
        "  - Test the trained model on the test environment\n",
        "  - Run the model on the environment with the actions generated by the Actor-Critic network until the game is done\n",
        "  - Return the total reward\n",
        "```"
      ],
      "metadata": {
        "id": "O6vDkxBZMKnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
        "  - Compute R = r0 + gamma * r1 + gamma^2 * r2 + ... for all rewards, apply mask to ignore rewards after the agent is done. The next_value is the last reward value\n",
        "  - Return the computed return values\n",
        "```"
      ],
      "metadata": {
        "id": "2rfNSiCqMyYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Run the following training loop and post training loop until the frame count of the agent playing the environment exceeds the max frame limit\n",
        "# Miltiple environment is used during data collection to speed up the process\n",
        "# Pre Training Loop\n",
        "- Reset the multiple environments, each environment will have slightly different starting state\n",
        "# Main Training Loop\n",
        "- For x number of steps, feed the state of each environment to the Actor-Critic network, and select the action from the action categorical probability\n",
        "- Apply the each action to its environment, and get the resulting reward for each environment\n",
        "- Store the reward, action log probability, entropy, potential reward, and mask for each environment. Mask is a list of 0s and 1s where its 0 at \"done\", and 1 otherwise. This acts to ignore rewards after the agent is done in the environment. Entropy is the measure of uncertainty and randomness in the distribution\n",
        "- Every y frames, run the test_env function above to test the performance of the current Actor-Critic network\n",
        "# Post Traning Loop\n",
        "- Continuing from the last state from each of the environment from the training loop, feed it to the Actor-Critic network and get the expected reward values; named \"next_value\" in the code\n",
        "- Use the \"next_value\", collected rewards, and masks to calculate the temporarily discounted return value from compute_returns function from above\n",
        "- Calculate the difference between the discounted returns values and collected expected values to get the advantage for each environment, which represents how well each environment performed with the current Actor compared to the expected return that the critic expected\n",
        "- Actor loss is calculated by the mean of each negated log probability multiplied by each advantage. Higher advantage results in higher loss, which means the optimizer will adjust the neural network more towards the direction of producing higher probability for high-reward actions\n",
        "- Critic loss is calculated by taking the mean of the square of the advantages. This represents the Mean Squared Error of the Critic network, where the network will try to minimize this value\n",
        "- Actor loss and Critic loss is combined, and subtract by entropy. Higher advantage leads to both higher Actor loss and Critic loss, which works for both Actor network and Critic network (Actor network wants to move more towards favoring high-reward actions, Critic network wants to adjust more for more wrong predictions).\n",
        "- Entropy is subtracted from the combined loss, because (1) exploration is encouraged. (2) High entropy means more uncertainty in the model choosing the actions. (2) Higher entropy decreases the loss value to maintain the current status-quo of exploratory nature\n",
        "- Apply backpropagation\n",
        "```"
      ],
      "metadata": {
        "id": "ePAmEpWFNBRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Why does higher advantage leads to higher loss, and why does higher loss lead to the model favoring thoses actions?\n",
        "# A higher positive advantage makes the loss term more positive.\n",
        "Since the optimizer aims to minimize this loss, it will adjust the parameters such that log p(a∣s) becomes less negative, effectively making p(a∣s) larger. This means the probability of taking that action a in state s will increase, making it more likely to be chosen in the future.\n",
        "```"
      ],
      "metadata": {
        "id": "0Atv-fulTzvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary modules\n",
        "import math\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if CUDA is available and set the device accordingly\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "# Importing SubprocVecEnv for parallelizing environments\n",
        "# from multiprocessing_env import SubprocVecEnv\n",
        "\n",
        "# Number of parallel environments and name of the environment\n",
        "num_envs = 8\n",
        "env_name = \"CartPole-v0\"\n",
        "\n",
        "# Function to create a new environment\n",
        "def make_env():\n",
        "    def _thunk():\n",
        "        env = gym.make(env_name)\n",
        "        return env\n",
        "    return _thunk\n",
        "\n",
        "# Enabling interactive mode for plotting\n",
        "plt.ion()\n",
        "\n",
        "# Create multiple environments for parallel execution\n",
        "envs = [make_env() for i in range(num_envs)]\n",
        "envs = SubprocVecEnv(envs)  # Create 8 parallel environments\n",
        "\n",
        "# Create a single environment for testing\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# Define the ActorCritic neural network model\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        # Define the critic network\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(num_inputs, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "\n",
        "        # Define the actor network\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(num_inputs, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, num_outputs),\n",
        "            nn.Softmax(dim=1),\n",
        "        )\n",
        "\n",
        "    # Forward pass for both actor and critic\n",
        "    def forward(self, x):\n",
        "        value = self.critic(x)\n",
        "        probs = self.actor(x)\n",
        "        dist = Categorical(probs)\n",
        "        return dist, value\n",
        "\n",
        "# Function to test the trained model in a single environment\n",
        "def test_env(vis=False):\n",
        "    state = env.reset()\n",
        "    if vis: env.render()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        dist, _ = model(state)\n",
        "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
        "        state = next_state\n",
        "        if vis: env.render()\n",
        "        total_reward += reward\n",
        "    return total_reward\n",
        "\n",
        "# Function to compute the discounted returns\n",
        "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
        "    R = next_value\n",
        "    returns = []\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        R = rewards[step] + gamma * R * masks[step]\n",
        "        returns.insert(0, R)\n",
        "    return returns\n",
        "\n",
        "# Function to plot rewards during training\n",
        "def plot(frame_idx, rewards):\n",
        "    plt.plot(rewards, 'b-')\n",
        "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
        "    plt.pause(0.0001)\n",
        "\n",
        "# Define model's input and output dimensions\n",
        "num_inputs = envs.observation_space.shape[0]\n",
        "num_outputs = envs.action_space.n\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 256\n",
        "lr = 1e-3\n",
        "num_steps = 5\n",
        "\n",
        "# Initialize the ActorCritic model and optimizer\n",
        "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training setup\n",
        "max_frames = 20000\n",
        "frame_idx = 0\n",
        "test_rewards = []\n",
        "\n",
        "# Initial state reset\n",
        "state = envs.reset()\n",
        "\n",
        "# Main training loop\n",
        "while frame_idx < max_frames:\n",
        "    # Initialize lists to store trajectories\n",
        "    log_probs = []\n",
        "    values = []\n",
        "    rewards = []\n",
        "    masks = []\n",
        "    entropy = 0\n",
        "\n",
        "    # Rollout trajectory for a few steps\n",
        "    for _ in range(num_steps):\n",
        "\n",
        "        # Convert the state to a PyTorch float tensor and move it to the configured device (CPU or GPU).\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "\n",
        "        # Get the action distribution ('dist') and state value ('value') from the Actor-Critic model.\n",
        "        dist, value = model(state)\n",
        "\n",
        "        # Sample an action from the distribution.\n",
        "        action = dist.sample()\n",
        "\n",
        "        # Take the action in the environment and receive the next state, reward, and 'done' flag.\n",
        "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
        "\n",
        "        # Calculate the log probability of the taken action.\n",
        "        log_prob = dist.log_prob(action)\n",
        "\n",
        "        # Compute the entropy for the action distribution to encourage exploration.\n",
        "        entropy += dist.entropy().mean()\n",
        "\n",
        "        # Append the log probability, state value, reward, and mask to their respective lists.\n",
        "        log_probs.append(log_prob)\n",
        "        values.append(value)\n",
        "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
        "\n",
        "        # Create a mask for the 'done' flag and append it to the masks list.\n",
        "        # The mask will be 0 if the episode is done, otherwise 1.\n",
        "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
        "\n",
        "        # Update the current state to be the next state.\n",
        "        state = next_state\n",
        "\n",
        "        # Increment the frame index.\n",
        "        frame_idx += 1\n",
        "\n",
        "        # Test and plot the performance every 100 frames.\n",
        "        if frame_idx % 100 == 0:\n",
        "\n",
        "            # Evaluate the model's performance in the environment and take the mean over 10 runs.\n",
        "            test_rewards.append(np.mean([test_env() for _ in range(10)]))\n",
        "\n",
        "            # Plot the accumulated test rewards.\n",
        "            plot(frame_idx, test_rewards)\n",
        "\n",
        "    # Compute discounted returns\n",
        "    next_state = torch.FloatTensor(next_state).to(device)\n",
        "    _, next_value = model(next_state)\n",
        "    returns = compute_returns(next_value, rewards, masks)\n",
        "\n",
        "    # Convert lists to PyTorch tensors\n",
        "    log_probs = torch.cat(log_probs)\n",
        "    returns = torch.cat(returns).detach()\n",
        "    values = torch.cat(values)\n",
        "\n",
        "    # Calculate advantage and losses\n",
        "    advantage = returns - values\n",
        "    actor_loss = -(log_probs * advantage.detach()).mean()\n",
        "    critic_loss = advantage.pow(2).mean()\n",
        "\n",
        "    # Calculate total loss and perform backpropagation\n",
        "    loss = actor_loss + 0.5 * critic_loss - 0.001 * entropy\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#test_env(True)"
      ],
      "metadata": {
        "id": "WH_3y61zIS_5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}