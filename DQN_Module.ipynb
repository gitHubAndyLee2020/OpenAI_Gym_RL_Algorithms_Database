{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQd4JS/oAD7be4npNR8Aww",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitHubAndyLee2020/OpenAI_Gym_RL_Algorithms_Database/blob/main/DQN_Module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DQN\n",
        "\n",
        "> About\n",
        "\n",
        "- Trains a evaluation network that predicts the potential reward for some given state and each action called Q-value\n",
        "- Has a copy of the evaluation network called target network. The target network is updated periodically, and it is used to stabilize training\n",
        "\n",
        "> Pro\n",
        "\n",
        "- Stability in Training; less oscillations and divergence during training\n",
        "\n",
        "> Con\n",
        "\n",
        "- Sample Inefficiency; needs large set of training data to learn effectively"
      ],
      "metadata": {
        "id": "rpIQn-NZfgoc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class Net():\n",
        "  def __init__(self):\n",
        "    - Initialize neural network\n",
        "\n",
        "  def forward(self, x):\n",
        "    - Forward the input through the neural network\n",
        "    - Return action probability of length number of actions; likelihood of taking those actions\n",
        "```"
      ],
      "metadata": {
        "id": "aZLnYuNDSOF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class DQN():\n",
        "  def __init__(self):\n",
        "    - Initialize two Net; one for evaluation network, and one for target network\n",
        "    - Initialize memory of length memory capacity that stores (state, action, reward, next state)\n",
        "    - Initialize optimizer and loss function for the evaluation network\n",
        "\n",
        "  def choose_action(self, state):\n",
        "    - If random number is less than or equal to some ε-greedy policy value, choose the action generated by the evaluation network. Otherwise, choose the action randomly\n",
        "    - This encourages explorations of taking novel actions\n",
        "\n",
        "  def store_transition(self, state, action, reward, next_state):\n",
        "    - Create transition object, which is a collection of state, action, reward, and next state\n",
        "    - Store the transition in the memory; if the memory is full, start replacing it from the earliest item in the current memory\n",
        "\n",
        "  def learn(self):\n",
        "    - Every Q Network Iteration step, store the weights of evaluation network into target network\n",
        "    - Otherwise, train the evaluation network\n",
        "    - First, select a batch of random transitions from the memory\n",
        "    - Then Q-Eval is calculated by getting the potential reward of taking an action using the evaluation network\n",
        "    - Similarly, Q-Target is calculated by batch reward + Gamma * Q-Next where Q-Next is the potential reward of taking an action using the target network on next state; this calculates the potential reward of current action and next action combined; target network is used to provide a more stable target that doesn't change alongside Q-Target\n",
        "    - The loss value is calculated between Q-Eval and Q-Target, which informs how much the evaluation network is off from the ideal target\n",
        "    - Backpropagation adjusts the weights of the evaluation network\n",
        "```"
      ],
      "metadata": {
        "id": "OVO1PXMYS8bF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def reward_func(env, x, x_dot, theta, theta_dor):\n",
        "  - Custom reward function, adjust to the environment as needed\n",
        "```"
      ],
      "metadata": {
        "id": "rLm4IB6Tczhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def main():\n",
        "  - Loop the training for some amount of epochs\n",
        "  - For each training loop, collect transitions from the environment from the start\n",
        "  - If the amount of stored transitions exceeds the memory capacity, start training the model\n",
        "  - If the agent is done in the environment, break out of the data collection and training loop\n",
        "```"
      ],
      "metadata": {
        "id": "3U02uFtObjNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Code - CartPole-v0"
      ],
      "metadata": {
        "id": "FTDTqLvJhBhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required libraries for pytorch, numpy, gym and matplotlib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "# Set hyper-parameters\n",
        "BATCH_SIZE = 128  # Size of the batch used in training\n",
        "LR = 0.01  # Learning rate\n",
        "GAMMA = 0.90  # Discount factor\n",
        "EPISILO = 0.9  # Epsilon for ε-greedy policy\n",
        "MEMORY_CAPACITY = 2000  # Capacity of replay buffer\n",
        "Q_NETWORK_ITERATION = 100  # Frequency of target network update\n",
        "\n",
        "# Create the gym environment (CartPole)\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "env = env.unwrapped  # Get the full environment\n",
        "NUM_ACTIONS = env.action_space.n  # Number of possible actions\n",
        "NUM_STATES = env.observation_space.shape[0]  # Number of state features\n",
        "# Check the shape of a sample action\n",
        "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample.shape\n",
        "\n",
        "# Define the neural network architecture for approximating Q-function\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()  # Initialize the superclass (nn.Module)\n",
        "        self.fc1 = nn.Linear(NUM_STATES, 50)  # First fully-connected layer\n",
        "        self.fc1.weight.data.normal_(0, 0.1)  # Initialize weights\n",
        "        self.fc2 = nn.Linear(50, 30)  # Second fully-connected layer\n",
        "        self.fc2.weight.data.normal_(0, 0.1)  # Initialize weights\n",
        "        self.out = nn.Linear(30, NUM_ACTIONS)  # Output layer\n",
        "        self.out.weight.data.normal_(0, 0.1)  # Initialize weights\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)  # First fully-connected layer\n",
        "        x = F.relu(x)  # ReLU activation\n",
        "        x = self.fc2(x)  # Second fully-connected layer\n",
        "        x = F.relu(x)  # ReLU activation\n",
        "        action_prob = self.out(x)  # Output layer\n",
        "        return action_prob  # Return the Q-values for each action\n",
        "\n",
        "# Define the DQN algorithm\n",
        "class DQN():\n",
        "    def __init__(self):\n",
        "        self.eval_net, self.target_net = Net(), Net()  # Initialize Q and target networks\n",
        "\n",
        "        self.learn_step_counter = 0  # For target updating\n",
        "        self.memory_counter = 0  # For storing memory\n",
        "        # Initialize memory (state, action, reward, next_state)\n",
        "        self.memory = np.zeros((MEMORY_CAPACITY, NUM_STATES * 2 + 2))\n",
        "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)  # Adam optimizer\n",
        "        self.loss_func = nn.MSELoss()  # Mean squared error loss\n",
        "\n",
        "    # Choose action based on state\n",
        "    def choose_action(self, state):\n",
        "        state = torch.unsqueeze(torch.FloatTensor(state), 0)  # Convert state to tensor\n",
        "        if np.random.randn() <= EPISILO:  # ε-greedy policy\n",
        "            action_value = self.eval_net.forward(state)\n",
        "            action = torch.max(action_value, 1)[1].data.numpy()\n",
        "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
        "        else:  # Random policy\n",
        "            action = np.random.randint(0, NUM_ACTIONS)\n",
        "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
        "        return action\n",
        "\n",
        "    # Store (s, a, r, s_) into memory\n",
        "    def store_transition(self, state, action, reward, next_state):\n",
        "        transition = np.hstack((state, [action, reward], next_state))  # Stack horizontally\n",
        "        index = self.memory_counter % MEMORY_CAPACITY  # Replace old memory with new memory\n",
        "        self.memory[index, :] = transition\n",
        "        self.memory_counter += 1\n",
        "\n",
        "    # Q-learning\n",
        "    def learn(self):\n",
        "        # Update the target network every Q_NETWORK_ITERATION times\n",
        "        if self.learn_step_counter % Q_NETWORK_ITERATION == 0:\n",
        "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
        "        self.learn_step_counter += 1\n",
        "\n",
        "        # Sample a mini-batch from memory\n",
        "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
        "        batch_memory = self.memory[sample_index, :]\n",
        "        batch_state = torch.FloatTensor(batch_memory[:, :NUM_STATES])\n",
        "        batch_action = torch.LongTensor(batch_memory[:, NUM_STATES:NUM_STATES + 1].astype(int))\n",
        "        batch_reward = torch.FloatTensor(batch_memory[:, NUM_STATES + 1:NUM_STATES + 2])\n",
        "        batch_next_state = torch.FloatTensor(batch_memory[:, -NUM_STATES:])\n",
        "\n",
        "        # Compute loss between Q-values and target Q-values\n",
        "        q_eval = self.eval_net(batch_state).gather(1, batch_action)\n",
        "        q_next = self.target_net(batch_next_state).detach()\n",
        "        q_target = batch_reward + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)\n",
        "        loss = self.loss_func(q_eval, q_target)\n",
        "\n",
        "        # Backpropagation\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "# Custom reward function for CartPole environment\n",
        "def reward_func(env, x, x_dot, theta, theta_dot):\n",
        "    r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.5\n",
        "    r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
        "    reward = r1 + r2\n",
        "    return reward\n",
        "\n",
        "# Main loop\n",
        "def main():\n",
        "    dqn = DQN()\n",
        "    episodes = 400\n",
        "    print(\"Collecting Experience....\")\n",
        "    reward_list = []\n",
        "    plt.ion()  # Turn on interactive mode for matplotlib\n",
        "    fig, ax = plt.subplots()\n",
        "    for i in range(episodes):\n",
        "        state = env.reset()  # Reset environment\n",
        "        ep_reward = 0  # Initialize episode reward\n",
        "        while True:\n",
        "            env.render()  # Render the environment\n",
        "            action = dqn.choose_action(state)  # Choose action\n",
        "            next_state, _ , done, _, info = env.step(action)  # Take action\n",
        "            x, x_dot, theta, theta_dot = next_state  # Components of next state\n",
        "            reward = reward_func(env, x, x_dot, theta, theta_dot)  # Compute custom reward\n",
        "            dqn.store_transition(state, action, reward, next_state)  # Store transition\n",
        "            ep_reward += reward  # Add reward to total episode reward\n",
        "\n",
        "            if dqn.memory_counter >= MEMORY_CAPACITY:  # If enough memory is collected\n",
        "                dqn.learn()  # Start learning\n",
        "                if done:  # If episode is done\n",
        "                    print(f\"episode: {i} , the episode reward is {round(ep_reward, 3)}\")\n",
        "            if done:  # If episode is done\n",
        "                break\n",
        "            state = next_state  # Update state\n",
        "\n",
        "        r = copy.copy(reward)  # Copy reward\n",
        "        reward_list.append(r)  # Append to reward list for plotting\n",
        "        ax.set_xlim(0, 300)  # X-axis limits\n",
        "        ax.plot(reward_list, 'g-', label='total_loss')  # Plotting\n",
        "        plt.pause(0.001)  # Pause to update plot\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "Gw1ZWtj7hA1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Code (Optimized) - CartPole-v0"
      ],
      "metadata": {
        "id": "6rzCJHxckJ5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import argparse\n",
        "import pickle\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "import os, time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal, Categorical\n",
        "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "# Set hyperparameters and environment variables\n",
        "seed = 1\n",
        "render = False\n",
        "num_episodes = 2000\n",
        "\n",
        "# Initialize the CartPole environment and set the state and action spaces\n",
        "env = gym.make('CartPole-v0').unwrapped\n",
        "num_state = env.observation_space.shape[0]\n",
        "num_action = env.action_space.n\n",
        "\n",
        "# Seed for reproducibility\n",
        "torch.manual_seed(seed)\n",
        "env.seed(seed)\n",
        "\n",
        "# Define the structure of a transition in experience replay\n",
        "Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state'])\n",
        "\n",
        "# Define the neural network for Q-Learning\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_state, 100)\n",
        "        self.fc2 = nn.Linear(100, num_action)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        action_value = self.fc2(x)\n",
        "        return action_value\n",
        "\n",
        "# Define the DQN agent\n",
        "class DQN():\n",
        "    # Initialize class variables\n",
        "    capacity = 8000\n",
        "    learning_rate = 1e-3\n",
        "    memory_count = 0\n",
        "    batch_size = 256\n",
        "    gamma = 0.995\n",
        "    update_count = 0\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DQN, self).__init__()\n",
        "        self.target_net, self.act_net = Net(), Net()\n",
        "        self.memory = [None]*self.capacity\n",
        "        self.optimizer = optim.Adam(self.act_net.parameters(), self.learning_rate)\n",
        "        self.loss_func = nn.MSELoss()\n",
        "        self.writer = SummaryWriter('./DQN/logs')\n",
        "\n",
        "    # Policy: Select action\n",
        "    def select_action(self,state):\n",
        "        state = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
        "        value = self.act_net(state)\n",
        "        action_max_value, index = torch.max(value, 1)\n",
        "        action = index.item()\n",
        "        if np.random.rand(1) >= 0.9:\n",
        "            action = np.random.choice(range(num_action), 1).item()\n",
        "        return action\n",
        "\n",
        "    # Store transitions for experience replay\n",
        "    def store_transition(self,transition):\n",
        "        index = self.memory_count % self.capacity\n",
        "        self.memory[index] = transition\n",
        "        self.memory_count += 1\n",
        "\n",
        "    # Update Q-values and policy\n",
        "    def update(self):\n",
        "        if self.memory_count >= self.capacity:\n",
        "            state = torch.tensor([t.state for t in self.memory]).float()\n",
        "            action = torch.LongTensor([t.action for t in self.memory]).view(-1,1).long()\n",
        "            reward = torch.tensor([t.reward for t in self.memory]).float()\n",
        "            next_state = torch.tensor([t.next_state for t in self.memory]).float()\n",
        "            reward = (reward - reward.mean()) / (reward.std() + 1e-7)\n",
        "            with torch.no_grad():\n",
        "                target_v = reward + self.gamma * self.target_net(next_state).max(1)[0]\n",
        "            for index in BatchSampler(SubsetRandomSampler(range(len(self.memory))), batch_size=self.batch_size, drop_last=False):\n",
        "                v = (self.act_net(state).gather(1, action))[index]\n",
        "                loss = self.loss_func(target_v[index].unsqueeze(1), v)\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                self.writer.add_scalar('loss/value_loss', loss, self.update_count)\n",
        "                self.update_count += 1\n",
        "                if self.update_count % 100 == 0:\n",
        "                    self.target_net.load_state_dict(self.act_net.state_dict())\n",
        "        else:\n",
        "            print(\"Memory Buff is too less\")\n",
        "\n",
        "# Main function to train the agent\n",
        "def main():\n",
        "    agent = DQN()\n",
        "    for i_ep in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        if render: env.render()\n",
        "        for t in range(10000):\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, _, info = env.step(action)\n",
        "            if render: env.render()\n",
        "            transition = Transition(state, action, reward, next_state)\n",
        "            agent.store_transition(transition)\n",
        "            state = next_state\n",
        "            if done or t >= 9999:\n",
        "                agent.writer.add_scalar('live/finish_step', t+1, global_step=i_ep)\n",
        "                agent.update()\n",
        "                if i_ep % 10 == 0:\n",
        "                    print(\"episodes {}, step is {} \".format(i_ep, t))\n",
        "                break\n",
        "\n",
        "# Entry point of the script\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "g38-XcsXkO4C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}