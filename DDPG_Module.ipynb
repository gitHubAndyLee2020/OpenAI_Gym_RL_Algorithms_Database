{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBuFliRbRMA7bbyydIG/P5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitHubAndyLee2020/OpenAI_Gym_RL_Algorithms_Database/blob/main/DDPG_Module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DDPG\n",
        "\n",
        "> About\n",
        "\n",
        "- Consists of Actor and Critic, where the Actor generates some continuous action for given state, and Critic generates expected reward for some state and action\n",
        "- Target Critic is used to stablish the training of Critic\n",
        "- Critic is first trained using actual reward and future predicted reward using Target Critic as target value, and then the updated Critic is used to train the Actor by judging its performance\n",
        "\n",
        "> Pro\n",
        "\n",
        "- Simplicity and Ease of Implementation\n",
        "\n",
        "> Con\n",
        "\n",
        "- Sample Inefficiency"
      ],
      "metadata": {
        "id": "P-enf0i1RpUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class Replay_buffer():\n",
        "  def __init__(self, max_size=capacity):\n",
        "    - Intialize the storage, maximum size, and pointer to keep track of current index\n",
        "\n",
        "  def push(self, data):\n",
        "    - If the length of the storage as reached the maximum size, start replacing the data from the oldest item in the storage\n",
        "    - Otherwise, append the data to the storage\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    - Select random batch_size items from the storage and return them\n",
        "```"
      ],
      "metadata": {
        "id": "jK-yxOIWBfw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class Actor(nn.Module):\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    - Map state input -> hidden layer -> action probability\n",
        "    - Also stores the maximum value for continuous action in variable self.max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    - Feed the input through the neural network, and apply tanh to the output of the neural network before multiplying it with self.max_action. Since tanh squeezes values to be between -1 and 1, the range of the forward function output is +-maximum action value\n",
        "```"
      ],
      "metadata": {
        "id": "8jCXRSRrC2-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class Critic(nn.Module):\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    - Map state input and action -> hidden layer -> expected reward value\n",
        "  \n",
        "  def forward(self, x, u):\n",
        "    - Feed the x: state and u: action through the neural network\n",
        "```"
      ],
      "metadata": {
        "id": "s40jy6L7F_Fa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class DDPG(object):\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    - Initialize the actor and target actor networks, and copy the weights of actor network to target actor network. Initialize the actor network optimizer\n",
        "    - Initialize the critic and target critic networks, and copy the weights of critic network to target critic network. Initialize the critic network optimizer\n",
        "    - Initialize the replay buffer\n",
        "\n",
        "  def select_action(self, state):\n",
        "    - Feed the state to actor network and get the action value\n",
        "    - Return the action value\n",
        "\n",
        "  def update(self):\n",
        "    - For the specified number of update iterations, run the following update loop\n",
        "    - Update loop is as follows\n",
        "    # Critic network update\n",
        "    - 1. Fetch a sample from the replay buffer and unwrap the state, action,next state, done, reward. These values are tensors of length batch_size\n",
        "    - 2. Compute the target Q value by feeding the state and action to the target critic network. The target Q value is the expected reward given some state and action. Then calculate the final target Q value using formula final target Q = reward + gamma * target Q value. This sets the target Q value that the critic network should aim to hit. The target critic network is used to stabilize training, as it avoids the target Q value from fluctuating alongside current Q value, leading to oscillation\n",
        "    - 3. Compute the current Q value by feeding the state and action to critic network\n",
        "    - 4. Calculate the loss between current Q value and target Q value\n",
        "    - 5. Apply backpropagation to critic network\n",
        "    # Actor network update\n",
        "    - 1. Generate action tensor by feeding state tensor to actor network\n",
        "    - 2. Feed the state tensor and generated action tensor to critic network to get the expected reward tensor\n",
        "    - 3. Get the negated mean of the expected reward tensor to get the Actor loss. This essentially calculates how much reward is expected from the actions chosen by the actor network, where if the expected reward is large, the Actor loss is smaller (due to negation) and requires less adjustment, whereas if the expected reward is small, the Acto is bigger and requires larger adjustment\n",
        "    - 4. Apply backpropagation\n",
        "    - Afterwards, the target networks are partially updated from main networks\n",
        "\n",
        "  def save(self):\n",
        "    - Save the weights of the actor and critic networks\n",
        "\n",
        "  def load(self):\n",
        "    - Load the weights into the actor and critic networks\n",
        "```"
      ],
      "metadata": {
        "id": "9-aLv75THSoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def main():\n",
        "  - If it is test model, run the agent for some number of iterations to get the number of achieved steps for each iteration\n",
        "  - Else if it is train model, for some number of iterations, collect trajectories, which is a tuple of state, next state, action, reward, done, and update the agent\n",
        "```"
      ],
      "metadata": {
        "id": "e8d8mZnoQEsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries and modules\n",
        "from itertools import count\n",
        "import os, sys, random\n",
        "import numpy as np\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "# Set various hyperparameters and settings for the algorithm\n",
        "mode = 'train'\n",
        "env_name = \"Pendulum-v1\"\n",
        "tau = 0.005\n",
        "target_update_interval = 1\n",
        "test_iteration = 10\n",
        "learning_rate = 1e-4\n",
        "gamma = 0.99\n",
        "capacity = 1000000\n",
        "batch_size = 100\n",
        "seed = False\n",
        "random_seed = 9527\n",
        "sample_frequency = 2000\n",
        "render = False\n",
        "log_interval = 50\n",
        "load = False\n",
        "render_interval = 100\n",
        "exploration_noise = 0.1\n",
        "max_episode = 100000\n",
        "print_log = 5\n",
        "update_iteration = 200\n",
        "\n",
        "# Detect if CUDA is available and set device accordingly\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Script name for identifying experiments\n",
        "script_name = \"ddpg\"\n",
        "\n",
        "# Initialize environment\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# Seed random numbers if 'seed' is True\n",
        "if seed:\n",
        "    env.seed(random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "# Extract state and action dimensions from environment\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "min_Val = torch.tensor(1e-7).float().to(device)\n",
        "\n",
        "# Directory for saving models and logging\n",
        "directory = './exp' + script_name + env_name +'./'\n",
        "\n",
        "# Class definition for Replay Buffer\n",
        "class Replay_buffer():\n",
        "    def __init__(self, max_size=capacity):\n",
        "        self.storage = []\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "\n",
        "    def push(self, data):\n",
        "        if len(self.storage) == self.max_size:\n",
        "            self.storage[int(self.ptr)] = data\n",
        "            self.ptr = (self.ptr + 1) % self.max_size\n",
        "        else:\n",
        "            self.storage.append(data)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "        x, y, u, r, d = [], [], [], [], []\n",
        "\n",
        "        for i in ind:\n",
        "            X, Y, U, R, D = self.storage[i]\n",
        "            x.append(np.array(X, copy=False))\n",
        "            y.append(np.array(Y, copy=False))\n",
        "            u.append(np.array(U, copy=False))\n",
        "            r.append(np.array(R, copy=False))\n",
        "            d.append(np.array(D, copy=False))\n",
        "\n",
        "        return np.array(x), np.array(y), np.array(u), np.array(r).reshape(-1, 1), np.array(d).reshape(-1, 1)\n",
        "\n",
        "# Class definition for Actor network\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, action_dim)\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = self.max_action * torch.tanh(self.l3(x))\n",
        "        return x\n",
        "\n",
        "# Class definition for Critic network\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.l2 = nn.Linear(400 , 300)\n",
        "        self.l3 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        x = F.relu(self.l1(torch.cat([x, u], 1)))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = self.l3(x)\n",
        "        return x\n",
        "\n",
        "# Main class definition for DDPG algorithm\n",
        "class DDPG(object):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
        "\n",
        "        self.critic = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "\n",
        "        self.replay_buffer = Replay_buffer()\n",
        "        self.writer = SummaryWriter(directory)\n",
        "\n",
        "        self.num_critic_update_iteration = 0\n",
        "        self.num_actor_update_iteration = 0\n",
        "        self.num_training = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "    def update(self):\n",
        "\n",
        "        for it in range(update_iteration):\n",
        "            # Sample replay buffer\n",
        "            x, y, u, r, d = self.replay_buffer.sample(batch_size)\n",
        "            state = torch.FloatTensor(x).to(device)\n",
        "            action = torch.FloatTensor(u).to(device)\n",
        "            next_state = torch.FloatTensor(y).to(device)\n",
        "            done = torch.FloatTensor(1-d).to(device)\n",
        "            reward = torch.FloatTensor(r).to(device)\n",
        "\n",
        "            # Compute the target Q value\n",
        "            target_Q = self.critic_target(next_state, self.actor_target(next_state))\n",
        "            target_Q = reward + (done * gamma * target_Q).detach()\n",
        "\n",
        "            # Get current Q estimate\n",
        "            current_Q = self.critic(state, action)\n",
        "\n",
        "            # Compute critic loss\n",
        "            critic_loss = F.mse_loss(current_Q, target_Q)\n",
        "            self.writer.add_scalar('Loss/critic_loss', critic_loss, global_step=self.num_critic_update_iteration)\n",
        "            # Optimize the critic\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.critic_optimizer.step()\n",
        "\n",
        "            # Compute actor loss\n",
        "            actor_loss = -self.critic(state, self.actor(state)).mean()\n",
        "            self.writer.add_scalar('Loss/actor_loss', actor_loss, global_step=self.num_actor_update_iteration)\n",
        "\n",
        "            # Optimize the actor\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optimizer.step()\n",
        "\n",
        "            # Update the frozen target models\n",
        "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "            self.num_actor_update_iteration += 1\n",
        "            self.num_critic_update_iteration += 1\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.actor.state_dict(), directory + 'actor.pth')\n",
        "        torch.save(self.critic.state_dict(), directory + 'critic.pth')\n",
        "\n",
        "    def load(self):\n",
        "        self.actor.load_state_dict(torch.load(directory + 'actor.pth'))\n",
        "        self.critic.load_state_dict(torch.load(directory + 'critic.pth'))\n",
        "        print(\"====================================\")\n",
        "        print(\"model has been loaded...\")\n",
        "        print(\"====================================\")\n",
        "\n",
        "# Main function for training or testing\n",
        "def main():\n",
        "    agent = DDPG(state_dim, action_dim, max_action)\n",
        "    ep_r = 0\n",
        "    if mode == 'test':\n",
        "        agent.load()\n",
        "        for i in range(test_iteration):\n",
        "            state = env.reset()\n",
        "            for t in count():\n",
        "                action = agent.select_action(state)\n",
        "                next_state, reward, done, info = env.step(np.float32(action))\n",
        "                ep_r += reward\n",
        "                env.render()\n",
        "                if done or t >= max_length_of_trajectory:\n",
        "                    print(\"Ep_i \\t{}, the ep_r is \\t{:0.2f}, the step is \\t{}\".format(i, ep_r, t))\n",
        "                    ep_r = 0\n",
        "                    break\n",
        "                state = next_state\n",
        "\n",
        "    elif mode == 'train':\n",
        "        if load: agent.load()\n",
        "        total_step = 0\n",
        "        for i in range(max_episode):\n",
        "            total_reward = 0\n",
        "            step =0\n",
        "            state = env.reset()\n",
        "            for t in count():\n",
        "                action = agent.select_action(state)\n",
        "                action = (action + np.random.normal(0, exploration_noise, size=env.action_space.shape[0])).clip(\n",
        "                    env.action_space.low, env.action_space.high)\n",
        "\n",
        "                next_state, reward, done, info = env.step(action)\n",
        "                if render and i >= render_interval : env.render()\n",
        "                agent.replay_buffer.push((state, next_state, action, reward, np.float(done)))\n",
        "\n",
        "                state = next_state\n",
        "                if done:\n",
        "                    break\n",
        "                step += 1\n",
        "                total_reward += reward\n",
        "            total_step += step+1\n",
        "            print(\"Total T:{} Episode: \\t{} Total Reward: \\t{:0.2f}\".format(total_step, i, total_reward))\n",
        "            agent.update()\n",
        "           # \"Total T: %d Episode Num: %d Episode T: %d Reward: %f\n",
        "\n",
        "            if i % log_interval == 0:\n",
        "                agent.save()\n",
        "    else:\n",
        "        raise NameError(\"mode wrong!!!\")\n",
        "\n",
        "# Start the program\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "HrcIIwVDIeC7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}