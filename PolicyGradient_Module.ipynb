{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOY/P+panFehHunW0SFb6Ub",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitHubAndyLee2020/OpenAI_Gym_RL_Algorithms_Database/blob/main/PolicyGradient_Module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PolicyGradient\n",
        "\n",
        "> About\n",
        "\n",
        "- Given some state, the Policy Gradient algorithm outputs the probability of taking each possible action\n",
        "- The neural network is trained to maximize the probability of action that produces the maximum reward\n",
        "\n",
        "> Pro\n",
        "\n",
        "- Better for Continuous and High-Dimensional Spaces\n",
        "\n",
        "> Con\n",
        "\n",
        "- Sample Inefficiency; requires many samples to train the agent\n"
      ],
      "metadata": {
        "id": "IZux_iENVAJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class Policy(nn.Module):\n",
        "  def __init__(self):\n",
        "    - Initialize a neural network that maps from input: observation length -> hidden layer: usually larger, something like 128 -> output: number of actions nodes\n",
        "    - Initialize storage for log probabilties and rewards\n",
        "\n",
        "  def forward(self, x):\n",
        "    - Feed the observation through the neural network and softmax to get action probabilities\n",
        "```"
      ],
      "metadata": {
        "id": "-brrIj61VdEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def select_action(state):\n",
        "  - Feed the state to the Policy network to get action probability\n",
        "  - Select an action from the action probabilty; if action had p probability in the action probabilty, then it has p chance of being selected\n",
        "  - Store the log of the probability of the action selected as well\n",
        "  - Return the selected action\n",
        "```"
      ],
      "metadata": {
        "id": "r1PoWQCsWUEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def finish_episode():\n",
        "  - Loop over the collected rewards and calculate the total reward, the reward per time step is calculated by R = r0 + gamma * r1 + gamma^2 * r2 + ..., this calculates the accumulated reward for the set of actions taken so far for each time step\n",
        "  - Normalize the reward history\n",
        "  - Calculate the policy loss by multipling the reward and negated action log probability of the action per each time step, where higher reward and higher action log probability will result in lower loss value. Lower loss value means the neural network will be adjusted more towards taking thoses actions during backpropagation\n",
        "  - With the calculated policy loss, backpropagation is applied to the Policy network\n",
        "```"
      ],
      "metadata": {
        "id": "iNFsNf8SYfFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def main():\n",
        "  - Collect action log probability and reward for some t amount of times\n",
        "  - Calculated running reward by the length of time steps the agent was able to survive in the environment, which the t from above\n",
        "  - If the running reward is higher than the environment's reward threshold, it means the agent is trained well. The training loop is terminated\n",
        "```"
      ],
      "metadata": {
        "id": "CZtZG0zseftf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries for environment handling and neural networks\n",
        "import gym\n",
        "import numpy as np\n",
        "from itertools import count\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Define constants and hyperparameters\n",
        "gamma = 0.99  # Discount factor for future rewards\n",
        "seed = 543  # Seed for reproducibility\n",
        "render = False  # Whether to render the environment\n",
        "log_interval = 10  # Logging interval for performance statistics\n",
        "\n",
        "# Create an environment object for the CartPole-v0 problem\n",
        "env = gym.make('CartPole-v0')\n",
        "# Seed the environment\n",
        "env.seed(seed)\n",
        "# Seed PyTorch for reproducibility\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Define the neural network architecture for the policy\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Policy, self).__init__()\n",
        "        # Define the first linear layer, input=4, output=128\n",
        "        self.affine1 = nn.Linear(4, 128)\n",
        "        # Define the second linear layer, input=128, output=2\n",
        "        self.affine2 = nn.Linear(128, 2)\n",
        "\n",
        "        # Initialize lists to store log probabilities and rewards\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "\n",
        "    # Define forward pass\n",
        "    def forward(self, x):\n",
        "        # Apply first linear layer followed by ReLU activation\n",
        "        x = F.relu(self.affine1(x))\n",
        "        # Apply second linear layer\n",
        "        action_scores = self.affine2(x)\n",
        "        # Apply softmax to get action probabilities\n",
        "        return F.softmax(action_scores, dim=1)\n",
        "\n",
        "# Instantiate the policy neural network\n",
        "policy = Policy()\n",
        "# Set up optimizer for the neural network\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
        "# Initialize epsilon to a small value for numerical stability\n",
        "eps = np.finfo(np.float32).eps.item()\n",
        "\n",
        "# Function to select an action given a state\n",
        "def select_action(state):\n",
        "    # Convert state to PyTorch tensor and add a batch dimension\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "    # Get action probabilities from the policy network\n",
        "    probs = policy(state)\n",
        "    # Create a categorical distribution over the action probabilities\n",
        "    m = Categorical(probs)\n",
        "    # Sample an action from this distribution\n",
        "    action = m.sample()\n",
        "    # Save the log probability of this action\n",
        "    policy.saved_log_probs.append(m.log_prob(action))\n",
        "    # Return the sampled action\n",
        "    return action.item()\n",
        "\n",
        "# Function to update the policy network\n",
        "def finish_episode():\n",
        "    # Initialize sum of rewards as zero\n",
        "    R = 0\n",
        "    # Initialize empty list to store policy loss values\n",
        "    policy_loss = []\n",
        "    # Initialize empty list to store future rewards\n",
        "    rewards = []\n",
        "    # Calculate the future reward for each time step\n",
        "    for r in policy.rewards[::-1]:\n",
        "        R = r + gamma * R\n",
        "        rewards.insert(0, R)\n",
        "    # Convert rewards to PyTorch tensor\n",
        "    rewards = torch.tensor(rewards)\n",
        "    # Normalize the rewards\n",
        "    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)\n",
        "    # Calculate policy loss\n",
        "    for log_prob, reward in zip(policy.saved_log_probs, rewards):\n",
        "        policy_loss.append(-log_prob * reward)\n",
        "    # Perform backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss = torch.cat(policy_loss).sum()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "    # Clear rewards and log probabilities for next episode\n",
        "    del policy.rewards[:]\n",
        "    del policy.saved_log_probs[:]\n",
        "\n",
        "# Main training loop\n",
        "def main():\n",
        "    # Initialize running reward\n",
        "    running_reward = 10\n",
        "    # Loop through episodes\n",
        "    for i_episode in count(1):\n",
        "        # Reset the environment for the new episode\n",
        "        state = env.reset()\n",
        "        # Loop through time steps in episode\n",
        "        for t in range(10000):  # Don't infinite loop while learning\n",
        "            # Select action using policy\n",
        "            action = select_action(state)\n",
        "            # Take action and observe next state and reward\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            # Optionally render the environment\n",
        "            if render:\n",
        "                env.render()\n",
        "            # Store reward\n",
        "            policy.rewards.append(reward)\n",
        "            # Break loop if episode ends\n",
        "            if done:\n",
        "                break\n",
        "        # Update running reward\n",
        "        running_reward = running_reward * 0.99 + t * 0.01\n",
        "        # Update policy\n",
        "        finish_episode()\n",
        "        # Log performance metrics\n",
        "        if i_episode % log_interval == 0:\n",
        "            print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
        "                i_episode, t, running_reward))\n",
        "        # Check if problem is solved\n",
        "        if running_reward > env.spec.reward_threshold:\n",
        "            print(\"Solved! Running reward is now {} and \"\n",
        "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
        "            break\n",
        "\n",
        "# Run the training loop if this script is executed\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "ZFtaWeQTW3BQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}