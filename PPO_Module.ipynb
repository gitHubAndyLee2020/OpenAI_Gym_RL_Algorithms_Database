{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMP/d0C+iOlLTggLHa5M952",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitHubAndyLee2020/OpenAI_Gym_RL_Algorithms_Database/blob/main/PPO_Module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PPO\n",
        "\n",
        "> About\n",
        "\n",
        "- Consists of Actor and Critic, where the Actor generates action probability for given state, and Critic generates expected return for given state\n",
        "- The Critic network is used to calculate the expected reward of taking some action sampled from training data, and the advantage is calculate by the difference between the expected reward and actual reward\n",
        "- For some state and action taken during training, ratio between the current updated Actor network selecting that action given the state divided by the original Actor network selecting that action is used to control the amount that the model is updated according to the advantage; this is used to prevent the updated network from deviating too much from the original network\n",
        "- The difference discounted actual reward value and predicted reward value is used to update the Critic network\n",
        "\n",
        "> Pro\n",
        "\n",
        "- Stability and Robustness\n",
        "\n",
        "> Con\n",
        "\n",
        "- Sample Inefficiency"
      ],
      "metadata": {
        "id": "-ON10-c8oQBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class Actor(nn.Module):\n",
        "  def __init__(self):\n",
        "    - Initialize neural network that maps state -> hidden layer -> action probability\n",
        "\n",
        "  def forward(self, x):\n",
        "    - Feed the input state through the neural network and apply softmax to the output action probability\n",
        "    - Return the action probability\n",
        "```"
      ],
      "metadata": {
        "id": "SLpG_6Aeccv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class Critic(nn.Module):\n",
        "  def __init__(self):\n",
        "    - Initialize neural network that maps state -> hidden layer -> expected reward\n",
        "    \n",
        "  def forward(self, x):\n",
        "    - Feed the input state through the neural network\n",
        "    - Return the expected reward\n",
        "```"
      ],
      "metadata": {
        "id": "sp4eRbRUc2MO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class PPO():\n",
        "  def __init__(self):\n",
        "    - Initialize Actor and Critic networks, Actor and Critic optimizers, and buffer to store training data\n",
        "\n",
        "  def select_action(self, state):\n",
        "    - Feed the state through the Actor network and get the action probability\n",
        "    - Convert the action probability into categorical probability and select an action\n",
        "    - Return the selected action and its action probability\n",
        "\n",
        "  def get_value(self, state):\n",
        "    - Feed the state through the Critic network\n",
        "    - Return the expected reward\n",
        "\n",
        "  def save_param(self):\n",
        "    - Save the Actor and Critic networks' weights\n",
        "\n",
        "  def store_transition(self, transition):\n",
        "    - Store the transition into the storage\n",
        "\n",
        "  def update(self, i_ep):\n",
        "    - Get the state tensor, action tensor, reward tensor, and action log probability tensor from transitions in the storage\n",
        "    - Calculate the discounted returns using R = r_cur + gamma * r_cur+1 + gamma^2 * r_cur+2 + ..., the discounted return is stored as a tensor [r0 + gamma * r1 + gamma^2 r2, r1 + gamma * r2 + gamma^2 * r3,...] for each time step, Gt represents how much reward the Actor model managed to achieve from the current time step to end of the game\n",
        "    - Run the following update loop for some amount of times\n",
        "    # Update Loop\n",
        "    - 1. Select a random index from the storage, item at the index is a batch of data\n",
        "    - 2. Fetch the Gt value at the index, this represents the actual reward achieved\n",
        "    - 3. Feed the state at the index to the Critic model, and get the expected reward\n",
        "    - 4. Take the difference between the Gt value and the expected reward. This represents how much better the Actor model performed compared to what was expected from the state, called the advantage\n",
        "    - 5. Feed the state into the Actor network, and get the probability of the action that was actually taken, let's call this generated action probability\n",
        "    - 6. Calculate the ratio by generated action probability / actual action probability. The ratio represents how much the Actor model changed compared to when the data was collect (in the first loop, the ratio will be close to 1 since the model hasn't been updated yet)\n",
        "    - 7. Calculate the first surrogate loss value by multiplying ratio and advantage. This represents how more likely is the updated Actor to choose the action that brings \"advantage\" amount of more rewards (\"advantage\" could be positive or negative)\n",
        "    - 8. Calculate the second surrogate loss value by clamping the ratio value to 1 +- clip paramter range then multiplying by the advantage. This achieves the same purpose as the first surrogate loss value, except the ratio is confined between hardline range limit, to avoid large loss value\n",
        "    - 9. Select the minimum value from the first and second surrogate loss values and take the negated mean (since all the values from the above are tensors from batches of data). The negation means that (1) high ratio and positive advantage -> low loss, less change, (2) high ratio and negative advantage -> high loss, more change; the model is directed towards favoring high-reward actions\n",
        "    - 10. Apply backpropagation to Actor network\n",
        "    - 11. Calculate the Critic loss by the Mean Square Loss of Gt value and expected value, more difference between Gt value (actual reward value) and expected value results in higher loss for more adjustments to the Critic network\n",
        "```"
      ],
      "metadata": {
        "id": "GXhIrVVxdKkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def main():\n",
        "  - Run the following training loop for some number of epochs\n",
        "  # Training loop\n",
        "  - Collect data from the environment until the game ends\n",
        "  - When the game is over, update the agent using the collected data\n",
        "```"
      ],
      "metadata": {
        "id": "F0JtiedXn1QB"
      }
    }
  ]
}